---
title: "BlackFriday_Pravat"
author: "Pravat"
date: "23 April 2019"
output:
  html_document: default
  pdf_document: default
---

# Global setup and Library import
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(data.table)
library(caret)          # To enable training with CV.
library(ggplot2)        # For visualization
library(e1071)          # For skewness
library(plyr)           # Iterative operations
```

# Introduction

The dataset here is a sample of the transactions made in a retail store. The store wants to know better the customer purchase behaviour against different products. Specifically, here the problem is a regression problem where we are trying to predict the dependent variable (the amount of purchase) with the help of the information contained in the other variables.

Classification problem can also be settled in this dataset since several variables are categorical, and some other approaches could be "Predicting the age of the consumer" or even "Predict the category of goods bought". This dataset is also particularly convenient for clustering and maybe find different clusters of consumers within it.

# CONTENT:

Both Dataset have the following fields, except Purchase - which is not available in BlackFriday_test.csv.

1. User_ID: Unique ID of the customer
2. Product_ID: Unique ID of the product sold/bought
3. Gender: Gender of the Customer
4. Age: Age group of the customer
5. Occupation: Customer occupation category
6. City_Category: Category of the city
7. Stay_In_Current_City_Years: Number of years the Customer has been staying in the city
8. Marital_Status: Customer's marital status
9. Product_Category_1: Parent category of the product
10. Product_Category_2: Sub-category on the Product_Category_1
11. Product_Category_3: Sub-category on the Product_Category_2
12. Purchase: Target variable. Monitary amount of purchase

# Useful Functions

```{r Useful functions}
# Function to split the dataset randomly with a given propert
splitdt <- function(dt, test_proportion=0.2, seed=NULL){
  if(!is.null(seed)) set.seed(seed)
  
  train_index <- sample(nrow(dt), floor(nrow(dt)*(1-test_proportion)), replace = FALSE)
  trainset<-dt[train_index]
  testset<-dt[-train_index]
  
  return(list(train=trainset, test=testset))
}

# MAPE metric
mape<-function(real,predicted){
  return(mean(abs((real-predicted)/real)))
}
```

# Data Reading and preparation

The dataset is offered in two separated fields, one for the training and another one for the test set. 

```{r Load Data}
original_training_data = fread(file = file.path("BlackFriday_train.csv"))
original_test_data = read.csv(file = file.path("BlackFriday_test.csv"))
```

To avoid applying the Feature Engineering process two times (once for training and once for test), you can just join both datasets (using the `rbind` function), apply your FE and then split the datasets again. However, if we try to do join the two dataframes as they are, we will get an error because they do not have the same columns: `test_data` does not have a column `Purchase`. Therefore, we first create this column in the test set and then we join the data

```{r Joinning datasets}
# Create the column Purchase in test set and assign the value to 0
original_test_data$Purchase <- 0

# Create the column dataType in both train and test set and assign the value 'train' & 'test'. This will help us to split the dataset from the dataType, not by position
original_training_data$dataType <- "train"
original_test_data$dataType <- "test"

# Join the two datasets
dataset <- rbind(original_training_data, original_test_data)
```

Let's now visualize the dataset to see where to begin
```{r Data Visualization}
summary(dataset)
```
We can see some problems just by taking a look to the summary: the dataset has missing values, there are some categorical columns codified as numeric, it has different scales for the feature values. In addition, we will take a deeper look to the data to detect more subtle issues: correlation between features, skewness in the feature values.


# EDA - Exploratory Data Analysis

Let's now visualize the dataset to get some insights that we can use during feature engineering.

## Distribution of Purchase by Gender

```{r Puchase vs Gender, echo=FALSE}
ggplot(original_training_data[, list(percent_purchase = round(sum(Purchase)/sum(original_training_data$Purchase), 2)), by = "Gender"], 
       aes(x="", y=percent_purchase, fill=Gender)) + 
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start=0) +
  labs(title= "Percentage Purchase by Gender")
```

This shows that purchases done by `Male` are more than 3 times of that done by `Female`.


## Purchase by Age group

```{r Puchase vs Age group, echo=FALSE}
purchase_age <- original_training_data[, 
                                       list(percentage_purchase_age = 100 * sum(Purchase)/sum(original_training_data$Purchase)), 
                                       by=c('Age')]

purchase_age[order(-percentage_purchase_age),]
ggplot(purchase_age, aes(Age, percentage_purchase_age), fill=Age) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Age") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Age group")
```

## Purchase by Occupation

```{r Puchase vs Occupation, echo=FALSE}
purchase_occupation <- original_training_data[, 
                                              list(percentage_purchase_occupation = 100 * sum(Purchase)/sum(original_training_data$Purchase)), 
                                              by=c('Occupation')]

purchase_occupation[order(-percentage_purchase_occupation),]

ggplot(purchase_occupation, aes(as.factor(Occupation), percentage_purchase_occupation), fill=Occupation) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Occupation") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Occupation")
```

## Purchase by City Category

```{r Puchase vs City Category, echo=FALSE}
purchase_city <- original_training_data[, 
                                        list(percentage_purchase_city = 100 * sum(Purchase)/sum(original_training_data$Purchase)), 
                                        by=c('City_Category')]

purchase_city[order(-percentage_purchase_city),]

ggplot(purchase_city, aes(City_Category, percentage_purchase_city), fill=City_Category) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("City_Category") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per City Category")
```

## Purchase by Stay_In_Current_City_Years

```{r Puchase vs Stay_In_Current_City_Years, echo=FALSE}
purchase_stay <- original_training_data[, 
                                        list(percentage_purchase_stay = 100 * sum(Purchase)/sum(original_training_data$Purchase)), 
                                        by=c('Stay_In_Current_City_Years')]

purchase_stay[order(-percentage_purchase_stay),]

ggplot(purchase_stay, aes(Stay_In_Current_City_Years, percentage_purchase_stay), fill=Stay_In_Current_City_Years) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Stay_In_Current_City_Years") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Stay_In_Current_City_Years")
```

## Purchase by Marital_Status

```{r Puchase vs Marital_Status, echo=FALSE}
purchase_mstatus <- original_training_data[, 
                                           list(percentage_purchase_mstatus = 100 * sum(Purchase)/sum(original_training_data$Purchase)), 
                                           by=c('Marital_Status')]

purchase_mstatus[order(-percentage_purchase_mstatus),]

ggplot(purchase_mstatus, aes(as.factor(Marital_Status), percentage_purchase_mstatus), fill=Marital_Status) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Marital_Status") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Marital_Status")
```

## Purchase by Product_Category_1

```{r Puchase vs Product_Category_1, echo=FALSE}
purchase_pcat1 <- original_training_data[, 
                                         list(percentage_purchase_pcat1 = 100 * sum(Purchase)/sum(original_training_data$Purchase)), 
                                         by=c('Product_Category_1')]

purchase_pcat1[order(-percentage_purchase_pcat1),]

ggplot(purchase_pcat1, aes(as.factor(Product_Category_1), percentage_purchase_pcat1), fill=Product_Category_1) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Product_Category_1") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Product_Category_1")
```

## Purchase by Product_Category_2

```{r Puchase vs Product_Category_2, echo=FALSE}
purchase_pcat2 <- original_training_data[, 
                                         list(percentage_purchase_pcat2 = 100 * sum(Purchase, na.rm = T)/sum(original_training_data$Purchase, na.rm = T)), 
                                         by=c('Product_Category_2')]

purchase_pcat2[order(-percentage_purchase_pcat2),]

ggplot(purchase_pcat2, aes(as.factor(Product_Category_2), percentage_purchase_pcat2), fill=Product_Category_2) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Product_Category_2") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Product_Category_2")
```

## Purchase by Product_Category_3

```{r Puchase vs Product_Category_3, echo=FALSE}
purchase_pcat3 <- original_training_data[, 
                                         list(percentage_purchase_pcat3 = 100 * sum(Purchase, na.rm = T)/sum(original_training_data$Purchase, na.rm = T)), 
                                         by=c('Product_Category_3')]

purchase_pcat3[order(-percentage_purchase_pcat3),]

ggplot(purchase_pcat3, aes(as.factor(Product_Category_3), percentage_purchase_pcat3), fill=Product_Category_3) +
  geom_col(col = "brown3") + 
  ylab("Percentage Purchase") + 
  xlab("Product_Category_3") + 
  theme_minimal() + 
  labs(title= "Percentage Purchase per Product_Category_3")
```

# Data Cleaning and Correction

## Factorize features

If we go back to the summary of the dataset we can identify some numerical features that are actually categories: `Occupation`, `Marital_Status`, `Product_Category_1`, `Product_Category_2` and `Product_Category_3`. What we have to do is to convert them to the proper 'class' or 'type' using the `as.factor` command.

```{r Factorize features}
# Declare the columns that needs to be converted to categorical variable
convert_to_cat_columns <- c("User_ID","Product_ID", "Occupation", "Marital_Status", "Product_Category_1", "Product_Category_2", "Product_Category_3", "dataType")

# Do the conversion
dataset[ , (convert_to_cat_columns) := lapply(.SD, as.factor), .SDcols = convert_to_cat_columns]
```


## Hunting NAs

Our dataset is filled with missing values, therefore, before we can build any predictive model we'll clean our data by filling in all NA's with more appropriate values. As another option, we could just remove the entries with null values (i.e., remove rows). However, in this situation (and in many other that you will face) this is out of the question: we have to provide a prediction for each and every record. Similarly, we could discard the features with null values (i.e., remove columns), but it would mean the removal of many features (and the information they provide).

As a rule of thumb, if we are allowed to discard some of your data and we do not have many null values (or you do not have a clear idea of how to impute them) we can safely delete them. If this is not the case, we must find a way to impute them (either by applying some knowledge of the addressed domain or by using some more advanced imputation method.

Counting columns with null values.

```{r NAs discovery}
na_cols <- names(which(colSums(is.na(dataset)) > 0))
paste('There are', length(na_cols), 'columns with missing values')

# Number of missing values in descending order
sort(
  colSums(
    sapply(
      dataset[, .SD, .SDcols = na_cols], 
      is.na)
  ), 
  decreasing = T)

# Missing values in percentages
sort(
  sapply(
    dataset[, .SD, .SDcols = na_cols], 
    function(x){100*sum(is.na(x))/length(x)}
  ), 
  decreasing = T);
```

Here we observe that `Product_Category_2` and `Product_Category_3` columns only have missing values. `Product_Category_3` is a sub-category of `Product_Category_2`, which inturn is a sub-category of `Product_Category_1`. So `Product_Category_1` sits at highest level, while `Product_Category_3` is the most ganular level. 

As of now, let's impute the missing values of these two columns with `UKN` i.e. Unknown.

```{r Impute NAs}
# Create a new level 'UKN' before we impute missing values with 'UKN'
dataset$Product_Category_2 = factor(dataset$Product_Category_2,
                                    levels=c(levels(dataset$Product_Category_2), "UKN"))
dataset$Product_Category_3 = factor(dataset$Product_Category_3, 
                                    levels=c(levels(dataset$Product_Category_3), "UKN"))

# Let's impute missing value now
dataset$Product_Category_2[is.na(dataset$Product_Category_2)] <- "UKN";
dataset$Product_Category_3[is.na(dataset$Product_Category_3)] <- "UKN";
```


## Outliers
We will now focus on numerical values. The main problem with numerical values are outliers (values which largely differ from the rest). Outliers can mislead the training of our models resulting in less accurate models and ultimately worse results.

In this dataset, we have `Purchase` i.e. the target variable as the only numeric column.

In this section we seek to identify outliers and then properly deal with them. If we summarize the dataset, we can see variables which "Max." is much larger than the rest of values. These features are susceptible of containing outliers. Nevetheless, the easiest way to detect outliers is visualizing the numerical values; for instance, by `boxploting` the column values.

The `boxplot` function can eliminate the outliers. However, if you apply it with the default values it is going to eliminate too much of them. We can adapt its working with the `outlier.size` param with a recommended value of at least 3.

```{r Outlier Detection}
# Split the dataset into train and test, so that we can remove outlier from training set
training_data <- dataset[dataset$dataType == "train",]
test_data <- dataset[dataset$dataType == "test",]

sprintf("Number of training records before outlier removal: %i", nrow(training_data))
par(mfrow=c(2,5))
for (col in names(training_data)) {
  if (!is.factor(training_data[[col]])){
    print(ggplot(training_data, aes_string(y=col)) + 
            geom_boxplot(width = 0.1, outlier.size = 3) + 
            theme(axis.line.x = element_blank(), axis.title.x = element_blank(), 
                  axis.ticks.x = element_blank(), axis.text.x = element_blank(),
                  legend.position="none"))
    
    #to_remove <- boxplot.stats(training_data[[col]])$out
    
    # I decided not to remove outliers as performance of the models went down
    #training_data <- training_data[!training_data[[col]] %in% to_remove, ]
  }
}
sprintf("Number of training records after outlier removal: %i", nrow(training_data))

# Merge the train and test
dataset <- rbind(training_data, test_data)
```

So, we see that `2403` number of records are identified as outliers and are removed from the training set. Once the outliers are removed from the training set, we again merge the training and test dataset so that we can apply feature engineering process on the whole dataset.


# Feature Engineering

This is the section to give free rein to our imagination and create/modify all the features that might improve the final result.

## Feature creation

```{r Feature creation}
training_data <- dataset[dataset$dataType == "train",]
test_data <- dataset[dataset$dataType == "test",]

# feature representing the count of each user
userIDCount <- as.data.table(table(training_data$User_ID))
colnames(userIDCount) <- c("User_ID","User_ID_Count")

training_data <- merge(x = training_data, y = userIDCount, by = "User_ID", all.x = TRUE)
test_data <- merge(x = test_data, y = userIDCount, by = "User_ID", all.x = TRUE)

sum(is.na(test_data$User_ID_Count))

# feature representing the count of each product
productIDCount <- as.data.table(table(training_data$Product_ID))
colnames(productIDCount) <- c("Product_ID","Product_ID_Count")

training_data <- merge(x = training_data, y = productIDCount, by = "Product_ID", all.x = TRUE)
test_data <- merge(x = test_data, y = productIDCount, by = "Product_ID", all.x = TRUE)

sum(is.na(test_data$Product_ID_Count))

# feature representing the average Purchase of each product
productIDMean <- ddply(training_data, .(Product_ID), summarize, Product_Mean=mean(Purchase))

training_data <- merge(x = training_data, y = productIDMean, by = "Product_ID", all.x = TRUE)
test_data <- merge(x = test_data, y = productIDMean, by = "Product_ID", all.x = TRUE)

sum(is.na(test_data$Product_Mean))
test_data$Product_Mean[is.na(test_data$Product_Mean)] <- mean(training_data$Purchase)

# feature representing the average Purchase by each user
userIDMean <- ddply(training_data, .(User_ID), summarize, User_Mean=mean(Purchase))

training_data <- merge(x = training_data, y = userIDMean, by = "User_ID", all.x = TRUE)
test_data <- merge(x = test_data, y = userIDMean, by = "User_ID", all.x = TRUE)

sum(is.na(test_data$Product_Mean))

# feature representing the proportion of times the user purchases the product more than the product's average
training_data$flag_high_product <- ifelse(training_data$Purchase > training_data$Product_Mean, 1, 0)
user_high_product <- ddply(training_data, .(User_ID), summarize, User_High_Product = mean(flag_high_product))

training_data <- merge(training_data, user_high_product, by="User_ID", all.x = TRUE)
test_data <- merge(test_data, user_high_product, by="User_ID", all.x = TRUE)

sum(is.na(test_data$User_High_Product))

training_data[, c("flag_high_product") := NULL]

# feature representing the proportion of times the user purchases more than the his/her own average
training_data$flag_high_user <- ifelse(training_data$Purchase > training_data$User_Mean, 1, 0)
user_high_self <- ddply(training_data, .(User_ID), summarize, User_High_Self = mean(flag_high_user))

training_data <- merge(training_data, user_high_self, by="User_ID", all.x = TRUE)
test_data <- merge(test_data, user_high_self, by="User_ID", all.x = TRUE)

sum(is.na(test_data$User_High_Self))

training_data[, c("flag_high_user") := NULL]

# Merge the train and test
dataset <- rbind(training_data, test_data)
```

## Feature transformation

```{r Feature transformation}
# Fill the unknown values in product category 2 and 3
na_idx_pcat_2 <- dataset$Product_Category_2 == "UKN"
dataset[na_idx_pcat_2,]$Product_Category_2 <- dataset[na_idx_pcat_2,]$Product_Category_1
dataset$Product_Category_2 <- as.factor(as.character(dataset$Product_Category_2))

na_idx_pcat_3 <- dataset$Product_Category_3 == "UKN"
dataset[na_idx_pcat_3,]$Product_Category_3 <- dataset[na_idx_pcat_3,]$Product_Category_2
dataset$Product_Category_3 <- as.factor(as.character(dataset$Product_Category_3))
```

# Skewness

While building predictive models we often see skewness in the target variable. Then we generally take transformations to make it more normal. We generally do it for linear models and not for tree based models. This actually means that our distribution is not normal, we are deliberately making it normal for prediction.

The way of getting rid of the skewness is to use the `log` (or the `log1p`) of the values of that feature, to flatten it. To reduce right skewness, take roots or logarithms or reciprocals (x to 1/x). This is the commonest problem in practice. To reduce left skewness, take squares or cubes or higher powers.

We now need to detect skewness in the Target value.

```{r Check skewness}
skewness(dataset$Purchase) #0.4091141 (Positive or right skewed, moderately skewed)

ggplot(dataset, aes(x = Purchase)) +
  geom_histogram()	
```

We can see that the traget variable has a skewness of around 0.4, which means that the data is moderately skewed. In such case, let's not take any action for skewness transformation.